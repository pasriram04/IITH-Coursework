{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8126437a0c214cc4b5f6d5a84fee147d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_80e10ea8f49d43549e70867441d32cca",
              "IPY_MODEL_f3219a34a6c74779be01e3e05a57888e",
              "IPY_MODEL_960302d6b9a245309ccd9111b1f78d3e"
            ],
            "layout": "IPY_MODEL_5fe02dbda66947ae8395aa855b4584e5"
          }
        },
        "80e10ea8f49d43549e70867441d32cca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5f2ad4a3b0c4c8b97760e7f4915f574",
            "placeholder": "​",
            "style": "IPY_MODEL_27911889a9344f6289474d2e55ea93f5",
            "value": "model.safetensors: 100%"
          }
        },
        "f3219a34a6c74779be01e3e05a57888e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d7d4c4e85ab4d63a260ec957ac06364",
            "max": 1115567652,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0066083e86dc496fbd3170a9968f8774",
            "value": 1115567652
          }
        },
        "960302d6b9a245309ccd9111b1f78d3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d7c8b8ef5af43d18ea98ffe0d434347",
            "placeholder": "​",
            "style": "IPY_MODEL_ec5190b1f40d46439ad415037774bd32",
            "value": " 1.12G/1.12G [00:09&lt;00:00, 247MB/s]"
          }
        },
        "5fe02dbda66947ae8395aa855b4584e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5f2ad4a3b0c4c8b97760e7f4915f574": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27911889a9344f6289474d2e55ea93f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d7d4c4e85ab4d63a260ec957ac06364": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0066083e86dc496fbd3170a9968f8774": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d7c8b8ef5af43d18ea98ffe0d434347": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec5190b1f40d46439ad415037774bd32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## English only"
      ],
      "metadata": {
        "id": "64uXughaQt5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# Hyperparameters and Config\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 2e-5\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "INCLUDE_DISGUST = False  # Set to False for English-only\n",
        "\n",
        "# Emotion labels\n",
        "emotions = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n",
        "if INCLUDE_DISGUST:\n",
        "    emotions.append('disgust')\n",
        "\n",
        "# Dataset Class\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.FloatTensor(label)\n",
        "        }\n",
        "\n",
        "# BERT-based Classifier\n",
        "class EmotionClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(EmotionClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        pooled_output = outputs.pooler_output\n",
        "        output = self.drop(pooled_output)\n",
        "        return self.out(output)\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    return np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, device):\n",
        "    model = model.eval()\n",
        "    predictions = []\n",
        "    real_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            real_labels.extend(labels.cpu().numpy())\n",
        "    return np.array(predictions), np.array(real_labels)\n",
        "\n",
        "# Data Loader\n",
        "def load_data(file_path, emotions):\n",
        "    df = pd.read_csv(file_path)\n",
        "    texts = df['text'].tolist()\n",
        "    labels = df[emotions].values\n",
        "    return texts, labels\n",
        "\n",
        "def main():\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "    train_texts, train_labels = load_data('train_eng.csv', emotions)\n",
        "    val_texts, val_labels = load_data('dev_eng.csv', emotions)\n",
        "\n",
        "    train_dataset = EmotionDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
        "    val_dataset = EmotionDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    model = EmotionClassifier(n_classes=len(emotions)).to(DEVICE)\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_loss = train_epoch(model, train_loader, loss_fn, optimizer, DEVICE)\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {train_loss:.4f}\")\n",
        "\n",
        "    preds, targets = eval_model(model, val_loader, DEVICE)\n",
        "    bin_preds = (preds >= 0.5).astype(int)\n",
        "\n",
        "    print(\"F1-macro:\", f1_score(targets, bin_preds, average='macro'))\n",
        "    print(\"Precision-macro:\", precision_score(targets, bin_preds, average='macro'))\n",
        "    print(\"Recall-macro:\", recall_score(targets, bin_preds, average='macro'))\n",
        "\n",
        "    print(\"\\nDetailed Classification Report:\")\n",
        "    print(classification_report(targets, bin_preds, target_names=emotions, zero_division=0))\n",
        "\n",
        "    torch.save(model.state_dict(), \"emotion_model.pt\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8ICESy0M5bk",
        "outputId": "3b745fd6-d21c-4e00-ff60-420727112cca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 0.5524\n",
            "Epoch 2/3, Loss: 0.4681\n",
            "Epoch 3/3, Loss: 0.3698\n",
            "F1-macro: 0.6423326225861329\n",
            "Precision-macro: 0.7417090384091819\n",
            "Recall-macro: 0.6058960573476703\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.86      0.38      0.52        16\n",
            "        fear       0.68      0.79      0.73        63\n",
            "         joy       0.76      0.42      0.54        31\n",
            "     sadness       0.71      0.83      0.76        35\n",
            "    surprise       0.70      0.61      0.66        31\n",
            "\n",
            "   micro avg       0.70      0.66      0.68       176\n",
            "   macro avg       0.74      0.61      0.64       176\n",
            "weighted avg       0.72      0.66      0.67       176\n",
            " samples avg       0.61      0.58      0.58       176\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Addition of\n",
        "\n",
        "- Focal Loss (for class imbalance)\n",
        "\n",
        "- Weighted Sampling (to oversample underrepresented emotions)\n",
        "\n",
        "- Per-Class Threshold Tuning (for best F1 per class)"
      ],
      "metadata": {
        "id": "B2tKcbOGwm_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, precision_recall_curve\n",
        "\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 2e-5\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "INCLUDE_DISGUST = False\n",
        "\n",
        "emotions = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n",
        "if INCLUDE_DISGUST:\n",
        "    emotions.append('disgust')\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.FloatTensor(label)\n",
        "        }\n",
        "\n",
        "class EmotionClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(EmotionClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        output = self.drop(pooled_output)\n",
        "        return self.out(output)\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = self.bce(inputs, targets)\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        focal_loss = self.alpha * ((1 - pt) ** self.gamma) * BCE_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    return np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, device):\n",
        "    model = model.eval()\n",
        "    predictions = []\n",
        "    real_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            real_labels.extend(labels.cpu().numpy())\n",
        "    return np.array(predictions), np.array(real_labels)\n",
        "\n",
        "def get_weighted_sampler(labels):\n",
        "    label_counts = np.sum(labels, axis=0)\n",
        "    class_weights = 1. / (label_counts + 1e-6)\n",
        "    sample_weights = []\n",
        "    for label in labels:\n",
        "        weight = np.sum(class_weights[label == 1])\n",
        "        sample_weights.append(weight)\n",
        "    return WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
        "\n",
        "def get_optimal_thresholds(preds, targets):\n",
        "    thresholds = []\n",
        "    for i in range(preds.shape[1]):\n",
        "        precision, recall, thresh = precision_recall_curve(targets[:, i], preds[:, i])\n",
        "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "        best_thresh = thresh[np.argmax(f1)] if len(thresh) > 0 else 0.5\n",
        "        thresholds.append(best_thresh)\n",
        "    return np.array(thresholds)\n",
        "\n",
        "def load_data(file_path, emotions):\n",
        "    df = pd.read_csv(file_path)\n",
        "    texts = df['text'].tolist()\n",
        "    labels = df[emotions].values\n",
        "    return texts, labels\n",
        "\n",
        "def main():\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "    train_texts, train_labels = load_data('train_eng.csv', emotions)\n",
        "    val_texts, val_labels = load_data('dev_eng.csv', emotions)\n",
        "\n",
        "    train_dataset = EmotionDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
        "    val_dataset = EmotionDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
        "\n",
        "    sampler = get_weighted_sampler(train_labels)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    model = EmotionClassifier(n_classes=len(emotions)).to(DEVICE)\n",
        "\n",
        "    loss_fn = FocalLoss(gamma=2)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_loss = train_epoch(model, train_loader, loss_fn, optimizer, DEVICE)\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {train_loss:.4f}\")\n",
        "\n",
        "    preds, targets = eval_model(model, val_loader, DEVICE)\n",
        "\n",
        "    thresholds = get_optimal_thresholds(preds, targets)\n",
        "    bin_preds = (preds >= thresholds).astype(int)\n",
        "\n",
        "    print(\"F1-macro:\", f1_score(targets, bin_preds, average='macro'))\n",
        "    print(\"Precision-macro:\", precision_score(targets, bin_preds, average='macro'))\n",
        "    print(\"Recall-macro:\", recall_score(targets, bin_preds, average='macro'))\n",
        "\n",
        "    print(\"\\nDetailed Classification Report:\")\n",
        "    print(classification_report(targets, bin_preds, target_names=emotions, zero_division=0))\n",
        "\n",
        "    torch.save(model.state_dict(), \"emotion_model.pt\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pmOtfq_sVhl",
        "outputId": "55c7c34b-b21b-4de8-e2a6-7ed773fcdf45"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 0.1468\n",
            "Epoch 2/3, Loss: 0.1029\n",
            "Epoch 3/3, Loss: 0.0751\n",
            "F1-macro: 0.6908847992665802\n",
            "Precision-macro: 0.7016922406746644\n",
            "Recall-macro: 0.7135176651305684\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.80      0.50      0.62        16\n",
            "        fear       0.65      0.95      0.77        63\n",
            "         joy       0.71      0.65      0.68        31\n",
            "     sadness       0.64      0.86      0.73        35\n",
            "    surprise       0.70      0.61      0.66        31\n",
            "\n",
            "   micro avg       0.67      0.78      0.72       176\n",
            "   macro avg       0.70      0.71      0.69       176\n",
            "weighted avg       0.68      0.78      0.71       176\n",
            " samples avg       0.66      0.70      0.66       176\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, precision_recall_curve\n",
        "\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 2e-5\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "INCLUDE_DISGUST = False\n",
        "\n",
        "emotions = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n",
        "if INCLUDE_DISGUST:\n",
        "    emotions.append('disgust')\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.FloatTensor(label)\n",
        "        }\n",
        "\n",
        "class EmotionClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(EmotionClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        output = self.drop(pooled_output)\n",
        "        return self.out(output)\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = self.bce(inputs, targets)\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        focal_loss = self.alpha * ((1 - pt) ** self.gamma) * BCE_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    return np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, device):\n",
        "    model = model.eval()\n",
        "    predictions = []\n",
        "    real_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            real_labels.extend(labels.cpu().numpy())\n",
        "    return np.array(predictions), np.array(real_labels)\n",
        "\n",
        "def get_weighted_sampler(labels):\n",
        "    label_counts = np.sum(labels, axis=0)\n",
        "    class_weights = 1. / (label_counts + 1e-6)\n",
        "    sample_weights = []\n",
        "    for label in labels:\n",
        "        weight = np.sum(class_weights[label == 1])\n",
        "        sample_weights.append(weight)\n",
        "    return WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
        "\n",
        "def get_optimal_thresholds(preds, targets):\n",
        "    thresholds = []\n",
        "    for i in range(preds.shape[1]):\n",
        "        precision, recall, thresh = precision_recall_curve(targets[:, i], preds[:, i])\n",
        "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "        best_thresh = thresh[np.argmax(f1)] if len(thresh) > 0 else 0.5\n",
        "        thresholds.append(best_thresh)\n",
        "    return np.array(thresholds)\n",
        "\n",
        "def load_data(file_path, emotions):\n",
        "    df = pd.read_csv(file_path)\n",
        "    texts = df['text'].tolist()\n",
        "    labels = df[emotions].values\n",
        "    return texts, labels\n",
        "\n",
        "def main():\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "    train_texts, train_labels = load_data('train_eng.csv', emotions)\n",
        "    val_texts, val_labels = load_data('dev_eng.csv', emotions)\n",
        "\n",
        "    train_dataset = EmotionDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
        "    val_dataset = EmotionDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
        "\n",
        "    sampler = get_weighted_sampler(train_labels)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    model = EmotionClassifier(n_classes=len(emotions)).to(DEVICE)\n",
        "\n",
        "    loss_fn = FocalLoss(gamma=2)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_loss = train_epoch(model, train_loader, loss_fn, optimizer, DEVICE)\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {train_loss:.4f}\")\n",
        "\n",
        "    preds, targets = eval_model(model, val_loader, DEVICE)\n",
        "\n",
        "    thresholds = get_optimal_thresholds(preds, targets)\n",
        "    bin_preds = (preds >= thresholds).astype(int)\n",
        "\n",
        "    print(\"F1-macro:\", f1_score(targets, bin_preds, average='macro'))\n",
        "    print(\"Precision-macro:\", precision_score(targets, bin_preds, average='macro'))\n",
        "    print(\"Recall-macro:\", recall_score(targets, bin_preds, average='macro'))\n",
        "\n",
        "    print(\"\\nDetailed Classification Report:\")\n",
        "    print(classification_report(targets, bin_preds, target_names=emotions, zero_division=0))\n",
        "\n",
        "    torch.save(model.state_dict(), \"emotion_model.pt\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ExGqIfKwr1g",
        "outputId": "27a8b705-8404-4680-fa69-e1159860297f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.1505\n",
            "Epoch 2/10, Loss: 0.1094\n",
            "Epoch 3/10, Loss: 0.0752\n",
            "Epoch 4/10, Loss: 0.0548\n",
            "Epoch 5/10, Loss: 0.0405\n",
            "Epoch 6/10, Loss: 0.0278\n",
            "Epoch 7/10, Loss: 0.0218\n",
            "Epoch 8/10, Loss: 0.0199\n",
            "Epoch 9/10, Loss: 0.0148\n",
            "Epoch 10/10, Loss: 0.0108\n",
            "F1-macro: 0.6766185007280898\n",
            "Precision-macro: 0.6518483576838736\n",
            "Recall-macro: 0.7288376856118791\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.73      0.50      0.59        16\n",
            "        fear       0.67      0.89      0.77        63\n",
            "         joy       0.52      0.77      0.62        31\n",
            "     sadness       0.77      0.77      0.77        35\n",
            "    surprise       0.56      0.71      0.63        31\n",
            "\n",
            "   micro avg       0.64      0.78      0.70       176\n",
            "   macro avg       0.65      0.73      0.68       176\n",
            "weighted avg       0.65      0.78      0.70       176\n",
            " samples avg       0.64      0.72      0.65       176\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## additions\n",
        "\n",
        "- Early stopping based on macro F1\n",
        "- Learning rate scheduling\n",
        "- Dynamic thresholding\n",
        "- Per-class loss monitoring\n",
        "- Hard negative mining\n",
        "\n",
        "## Conceptual Explanation: Hard Negative Mining & Dynamic Thresholding\n",
        "\n",
        "### Hard Negative Mining (via Focal Loss)\n",
        "\n",
        "- Hard negative mining is a technique used to focus learning on difficult examples specifically, the ones the model is currently misclassifying or struggling with.\n",
        "\n",
        "In this implementation, we achieve hard negative mining using **Focal Loss**. Here's how it works:\n",
        "\n",
        "- Standard loss functions (like Binary Cross Entropy) treat all samples equally.\n",
        "- Focal Loss, however, **down-weights well-classified examples** and focuses more on hard, misclassified ones.\n",
        "- This is done using the term:\n",
        "\n",
        "  $$\n",
        "  \\text{Focal Loss} = \\alpha \\cdot (1 - p_t)^\\gamma \\cdot \\text{BCE}\n",
        "  $$\n",
        "\n",
        "  where:\n",
        "  - $ p_t $ is the model’s predicted probability for the true class,\n",
        "  - $ \\gamma $ is a focusing parameter (typically ≥ 2),\n",
        "  - $ \\alpha $ is a weighting factor.\n",
        "\n",
        "*** In Summary: easy samples are ignored, and the model spends more effort on learning from difficult \\(hard negative\\) examples ***.\n",
        "\n",
        "\n",
        "\n",
        "### Dynamic Thresholding\n",
        "\n",
        "- In multi-label classification, each label (emotion, in this case) is predicted independently using a sigmoid output. The common practice is to use a fixed threshold (like 0.5) to decide whether a label is active.\n",
        "\n",
        "- However, that approach is not highly recommended, especially when class imbalance or varying decision boundaries exist.\n",
        "\n",
        "To address this, Dynamic Thresholding does the following:\n",
        "\n",
        "1. For each class, it computes the Precision-Recall curve using validation predictions.\n",
        "2. It then calculates the F1 score at different thresholds.\n",
        "3. Finally, it selects the optimal threshold that gives the highest F1 for each class.\n",
        "\n",
        "This leads to:\n",
        "- Better precision-recall balance per class,\n",
        "- Improved macro-F1 score, which is the main metric used for early stopping and learning rate scheduling.\n",
        "\n",
        "*** In Summary: instead of using a one-size-fits-all threshold, we adaptively choose the best one per class, based on validation performance. ***\n"
      ],
      "metadata": {
        "id": "JOHrp7l8P2CZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, precision_recall_curve\n",
        "\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 2e-5\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "INCLUDE_DISGUST = False\n",
        "EARLY_STOPPING_PATIENCE = 3\n",
        "\n",
        "emotions = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n",
        "if INCLUDE_DISGUST:\n",
        "    emotions.append('disgust')\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.FloatTensor(label)\n",
        "        }\n",
        "\n",
        "class EmotionClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(EmotionClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        output = self.drop(pooled_output)\n",
        "        return self.out(output)\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = self.bce(inputs, targets)\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        focal_loss = self.alpha * ((1 - pt) ** self.gamma) * BCE_loss\n",
        "        return focal_loss.mean(dim=0), focal_loss.mean()\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
        "    model = model.train()\n",
        "    total_loss = 0\n",
        "    per_class_loss = torch.zeros(len(emotions)).to(device)\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss_per_class, loss = loss_fn(outputs, labels)\n",
        "        per_class_loss += loss_per_class.detach()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader), (per_class_loss / len(data_loader)).cpu().numpy()\n",
        "\n",
        "def eval_model(model, data_loader, device):\n",
        "    model = model.eval()\n",
        "    predictions = []\n",
        "    real_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            real_labels.extend(labels.cpu().numpy())\n",
        "    return np.array(predictions), np.array(real_labels)\n",
        "\n",
        "def get_weighted_sampler(labels):\n",
        "    label_counts = np.sum(labels, axis=0)\n",
        "    class_weights = 1. / (label_counts + 1e-6)\n",
        "    sample_weights = []\n",
        "    for label in labels:\n",
        "        weight = np.sum(class_weights[label == 1])\n",
        "        sample_weights.append(weight)\n",
        "    return WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
        "\n",
        "def get_optimal_thresholds(preds, targets):\n",
        "    thresholds = []\n",
        "    for i in range(preds.shape[1]):\n",
        "        precision, recall, thresh = precision_recall_curve(targets[:, i], preds[:, i])\n",
        "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "        best_thresh = thresh[np.argmax(f1)] if len(thresh) > 0 else 0.5\n",
        "        thresholds.append(best_thresh)\n",
        "    return np.array(thresholds)\n",
        "\n",
        "def load_data(file_path, emotions):\n",
        "    df = pd.read_csv(file_path)\n",
        "    texts = df['text'].tolist()\n",
        "    labels = df[emotions].values\n",
        "    return texts, labels\n",
        "\n",
        "def main():\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "    train_texts, train_labels = load_data('train_eng.csv', emotions)\n",
        "    val_texts, val_labels = load_data('dev_eng.csv', emotions)\n",
        "\n",
        "    train_dataset = EmotionDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
        "    val_dataset = EmotionDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
        "\n",
        "    sampler = get_weighted_sampler(train_labels)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    model = EmotionClassifier(n_classes=len(emotions)).to(DEVICE)\n",
        "    loss_fn = FocalLoss(gamma=2)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=1, verbose=True)\n",
        "\n",
        "    best_macro_f1 = 0\n",
        "    patience = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_loss, class_losses = train_epoch(model, train_loader, loss_fn, optimizer, DEVICE)\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {train_loss:.4f}, Per-class Loss: {class_losses}\")\n",
        "\n",
        "        preds, targets = eval_model(model, val_loader, DEVICE)\n",
        "        thresholds = get_optimal_thresholds(preds, targets)\n",
        "        bin_preds = (preds >= thresholds).astype(int)\n",
        "\n",
        "        macro_f1 = f1_score(targets, bin_preds, average='macro')\n",
        "        scheduler.step(macro_f1)\n",
        "\n",
        "        print(f\"F1-macro: {macro_f1}\")\n",
        "        print(\"Precision-macro:\", precision_score(targets, bin_preds, average='macro'))\n",
        "        print(\"Recall-macro:\", recall_score(targets, bin_preds, average='macro'))\n",
        "        print(\"Detailed Classification Report:\")\n",
        "        print(classification_report(targets, bin_preds, target_names=emotions, zero_division=0))\n",
        "\n",
        "        if macro_f1 > best_macro_f1:\n",
        "            best_macro_f1 = macro_f1\n",
        "            patience = 0\n",
        "            torch.save(model.state_dict(), \"best_emotion_model.pt\")\n",
        "        else:\n",
        "            patience += 1\n",
        "            if patience >= EARLY_STOPPING_PATIENCE:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTrqi1hTx2aI",
        "outputId": "4e1707d8-b372-4f6c-bc76-ae675d74ac01"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.1481, Per-class Loss: [0.1430554  0.1489459  0.13340986 0.16556878 0.14931847]\n",
            "F1-macro: 0.6473179773179774\n",
            "Precision-macro: 0.6386451425188872\n",
            "Recall-macro: 0.6809114183307731\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.73      0.50      0.59        16\n",
            "        fear       0.66      0.89      0.76        63\n",
            "         joy       0.51      0.65      0.57        31\n",
            "     sadness       0.73      0.63      0.68        35\n",
            "    surprise       0.56      0.74      0.64        31\n",
            "\n",
            "   micro avg       0.63      0.73      0.68       176\n",
            "   macro avg       0.64      0.68      0.65       176\n",
            "weighted avg       0.64      0.73      0.67       176\n",
            " samples avg       0.63      0.68      0.63       176\n",
            "\n",
            "Epoch 2/10, Loss: 0.1021, Per-class Loss: [0.08529981 0.10950178 0.0896517  0.12064811 0.10524616]\n",
            "F1-macro: 0.6877745103097216\n",
            "Precision-macro: 0.6510110197121767\n",
            "Recall-macro: 0.7560432667690733\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.75      0.56      0.64        16\n",
            "        fear       0.70      0.87      0.77        63\n",
            "         joy       0.62      0.65      0.63        31\n",
            "     sadness       0.67      0.83      0.74        35\n",
            "    surprise       0.51      0.87      0.64        31\n",
            "\n",
            "   micro avg       0.64      0.80      0.71       176\n",
            "   macro avg       0.65      0.76      0.69       176\n",
            "weighted avg       0.65      0.80      0.71       176\n",
            " samples avg       0.63      0.73      0.65       176\n",
            "\n",
            "Epoch 3/10, Loss: 0.0700, Per-class Loss: [0.04672356 0.09080095 0.057411   0.08508096 0.07003781]\n",
            "F1-macro: 0.6817686582333973\n",
            "Precision-macro: 0.7447963800904978\n",
            "Recall-macro: 0.6552764976958525\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.75      0.38      0.50        16\n",
            "        fear       0.69      0.86      0.77        63\n",
            "         joy       0.75      0.68      0.71        31\n",
            "     sadness       0.88      0.66      0.75        35\n",
            "    surprise       0.65      0.71      0.68        31\n",
            "\n",
            "   micro avg       0.72      0.72      0.72       176\n",
            "   macro avg       0.74      0.66      0.68       176\n",
            "weighted avg       0.74      0.72      0.71       176\n",
            " samples avg       0.68      0.68      0.65       176\n",
            "\n",
            "Epoch 4/10, Loss: 0.0540, Per-class Loss: [0.03081668 0.06703566 0.05055797 0.06787498 0.05346566]\n",
            "F1-macro: 0.6751170378460958\n",
            "Precision-macro: 0.6269567275261491\n",
            "Recall-macro: 0.7434408602150537\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.62      0.50      0.55        16\n",
            "        fear       0.66      0.90      0.77        63\n",
            "         joy       0.59      0.71      0.65        31\n",
            "     sadness       0.63      0.83      0.72        35\n",
            "    surprise       0.63      0.77      0.70        31\n",
            "\n",
            "   micro avg       0.64      0.80      0.71       176\n",
            "   macro avg       0.63      0.74      0.68       176\n",
            "weighted avg       0.63      0.80      0.70       176\n",
            " samples avg       0.65      0.74      0.66       176\n",
            "\n",
            "Epoch 5/10, Loss: 0.0327, Per-class Loss: [0.01512793 0.04494277 0.02731499 0.0428157  0.03344469]\n",
            "F1-macro: 0.6867583929554211\n",
            "Precision-macro: 0.647828394515701\n",
            "Recall-macro: 0.7451472094214029\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.64      0.56      0.60        16\n",
            "        fear       0.62      0.94      0.75        63\n",
            "         joy       0.61      0.74      0.67        31\n",
            "     sadness       0.76      0.74      0.75        35\n",
            "    surprise       0.61      0.74      0.67        31\n",
            "\n",
            "   micro avg       0.64      0.80      0.71       176\n",
            "   macro avg       0.65      0.75      0.69       176\n",
            "weighted avg       0.65      0.80      0.71       176\n",
            " samples avg       0.65      0.74      0.66       176\n",
            "\n",
            "Early stopping triggered.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More additions"
      ],
      "metadata": {
        "id": "Wte7Z-c_Tsmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Switched to BCEWithLogitsLoss with class-wise alpha weighting. So FocalLoss now supports class-wise alpha based on inverse class frequency.\n",
        "# Replaced ReduceLROnPlateau with CosineAnnealingLR.\n",
        "# Gradient clipping.\n",
        "# Thresholds are printed.\n",
        "# DataLoader shuffling when the sampler is not being used.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, precision_recall_curve\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "set_seed()\n",
        "\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 2e-5\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "INCLUDE_DISGUST = False\n",
        "EARLY_STOPPING_PATIENCE = 3\n",
        "GRAD_CLIP = 1.0\n",
        "\n",
        "emotions = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n",
        "if INCLUDE_DISGUST:\n",
        "    emotions.append('disgust')\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'labels': torch.FloatTensor(label)\n",
        "        }\n",
        "\n",
        "class EmotionClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(EmotionClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        return self.out(self.drop(pooled_output))\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = torch.tensor(alpha).float() if alpha is not None else None\n",
        "        self.gamma = gamma\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = self.bce(inputs, targets)\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        focal_loss = ((1 - pt) ** self.gamma) * BCE_loss\n",
        "        if self.alpha is not None:\n",
        "            focal_loss = self.alpha.to(inputs.device) * focal_loss\n",
        "        return focal_loss.mean(dim=0), focal_loss.mean()\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    per_class_loss = torch.zeros(len(emotions)).to(device)\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss_per_class, loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        per_class_loss += loss_per_class.detach()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader), (per_class_loss / len(data_loader)).cpu().numpy()\n",
        "\n",
        "def eval_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    real_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            real_labels.extend(labels.cpu().numpy())\n",
        "    return np.array(predictions), np.array(real_labels)\n",
        "\n",
        "def get_weighted_sampler(labels):\n",
        "    label_counts = np.sum(labels, axis=0)\n",
        "    class_weights = 1. / (label_counts + 1e-6)\n",
        "    sample_weights = np.dot(labels, class_weights)\n",
        "    return WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True), class_weights\n",
        "\n",
        "def get_optimal_thresholds(preds, targets):\n",
        "    thresholds = []\n",
        "    for i in range(preds.shape[1]):\n",
        "        precision, recall, thresh = precision_recall_curve(targets[:, i], preds[:, i])\n",
        "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "        best_thresh = thresh[np.argmax(f1)] if len(thresh) > 0 else 0.5\n",
        "        thresholds.append(best_thresh)\n",
        "    return np.array(thresholds)\n",
        "\n",
        "def load_data(file_path, emotions):\n",
        "    df = pd.read_csv(file_path)\n",
        "    texts = df['text'].tolist()\n",
        "    labels = df[emotions].values\n",
        "    return texts, labels\n",
        "\n",
        "def main():\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "    train_texts, train_labels = load_data('train_eng.csv', emotions)\n",
        "    val_texts, val_labels = load_data('dev_eng.csv', emotions)\n",
        "\n",
        "    sampler, alpha = get_weighted_sampler(train_labels)\n",
        "    train_dataset = EmotionDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
        "    val_dataset = EmotionDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    model = EmotionClassifier(n_classes=len(emotions)).to(DEVICE)\n",
        "    loss_fn = FocalLoss(alpha=alpha, gamma=2)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "    best_macro_f1 = 0\n",
        "    patience = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_loss, class_losses = train_epoch(model, train_loader, loss_fn, optimizer, DEVICE)\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {train_loss:.4f}, Per-class Loss: {class_losses}\")\n",
        "\n",
        "        preds, targets = eval_model(model, val_loader, DEVICE)\n",
        "        thresholds = get_optimal_thresholds(preds, targets)\n",
        "        print(\"Thresholds:\", thresholds)\n",
        "        bin_preds = (preds >= thresholds).astype(int)\n",
        "\n",
        "        macro_f1 = f1_score(targets, bin_preds, average='macro')\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"F1-macro: {macro_f1}\")\n",
        "        print(\"Precision-macro:\", precision_score(targets, bin_preds, average='macro'))\n",
        "        print(\"Recall-macro:\", recall_score(targets, bin_preds, average='macro'))\n",
        "        print(\"Classification Report:\")\n",
        "        print(classification_report(targets, bin_preds, target_names=emotions, zero_division=0))\n",
        "\n",
        "        if macro_f1 > best_macro_f1:\n",
        "            best_macro_f1 = macro_f1\n",
        "            patience = 0\n",
        "            torch.save(model.state_dict(), \"best_emotion_model.pt\")\n",
        "        else:\n",
        "            patience += 1\n",
        "            if patience >= EARLY_STOPPING_PATIENCE:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DA_Q1zTg3Uip",
        "outputId": "41b78fca-8435-477c-a02c-b8b2c8fba0f0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.0002, Per-class Loss: [4.2069220e-04 9.8013625e-05 2.0450109e-04 1.8887612e-04 1.9408338e-04]\n",
            "Thresholds: [0.44262043 0.4875288  0.32372165 0.53325963 0.5517741 ]\n",
            "F1-macro: 0.6533623643887403\n",
            "Precision-macro: 0.609249788152051\n",
            "Recall-macro: 0.7654377880184332\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.67      0.50      0.57        16\n",
            "        fear       0.57      1.00      0.73        63\n",
            "         joy       0.43      0.94      0.59        31\n",
            "     sadness       0.81      0.71      0.76        35\n",
            "    surprise       0.57      0.68      0.62        31\n",
            "\n",
            "   micro avg       0.57      0.83      0.67       176\n",
            "   macro avg       0.61      0.77      0.65       176\n",
            "weighted avg       0.60      0.83      0.68       176\n",
            " samples avg       0.58      0.76      0.63       176\n",
            "\n",
            "Epoch 2/10, Loss: 0.0001, Per-class Loss: [2.0852283e-04 8.3534542e-05 1.4172826e-04 1.4558583e-04 1.4082473e-04]\n",
            "Thresholds: [0.46822497 0.5083664  0.48325822 0.5747574  0.6033791 ]\n",
            "F1-macro: 0.6727604442094129\n",
            "Precision-macro: 0.6885312899106002\n",
            "Recall-macro: 0.6673732718894009\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.53      0.50      0.52        16\n",
            "        fear       0.67      0.86      0.75        63\n",
            "         joy       0.67      0.65      0.66        31\n",
            "     sadness       0.85      0.66      0.74        35\n",
            "    surprise       0.72      0.68      0.70        31\n",
            "\n",
            "   micro avg       0.69      0.72      0.70       176\n",
            "   macro avg       0.69      0.67      0.67       176\n",
            "weighted avg       0.70      0.72      0.70       176\n",
            " samples avg       0.65      0.66      0.63       176\n",
            "\n",
            "Epoch 3/10, Loss: 0.0001, Per-class Loss: [1.09197936e-04 7.27939332e-05 9.47141380e-05 1.21718833e-04\n",
            " 1.11043300e-04]\n",
            "Thresholds: [0.5916212  0.5707778  0.45731792 0.3132084  0.65533084]\n",
            "F1-macro: 0.641149964323377\n",
            "Precision-macro: 0.6273286004771549\n",
            "Recall-macro: 0.6810343061955965\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.62      0.50      0.55        16\n",
            "        fear       0.74      0.79      0.76        63\n",
            "         joy       0.54      0.65      0.59        31\n",
            "     sadness       0.53      0.89      0.66        35\n",
            "    surprise       0.72      0.58      0.64        31\n",
            "\n",
            "   micro avg       0.63      0.72      0.67       176\n",
            "   macro avg       0.63      0.68      0.64       176\n",
            "weighted avg       0.65      0.72      0.67       176\n",
            " samples avg       0.62      0.65      0.61       176\n",
            "\n",
            "Epoch 4/10, Loss: 0.0001, Per-class Loss: [4.8834710e-05 6.6851171e-05 7.2329953e-05 9.5783274e-05 8.1168706e-05]\n",
            "Thresholds: [0.19522764 0.50962454 0.4064393  0.4980097  0.65061957]\n",
            "F1-macro: 0.6767248960190135\n",
            "Precision-macro: 0.6521750879839116\n",
            "Recall-macro: 0.7146915002560162\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.53      0.56      0.55        16\n",
            "        fear       0.64      0.92      0.76        63\n",
            "         joy       0.66      0.68      0.67        31\n",
            "     sadness       0.70      0.80      0.75        35\n",
            "    surprise       0.73      0.61      0.67        31\n",
            "\n",
            "   micro avg       0.66      0.77      0.71       176\n",
            "   macro avg       0.65      0.71      0.68       176\n",
            "weighted avg       0.66      0.77      0.70       176\n",
            " samples avg       0.67      0.70      0.66       176\n",
            "\n",
            "Epoch 5/10, Loss: 0.0001, Per-class Loss: [2.9228922e-05 5.6860354e-05 5.0554412e-05 7.8300538e-05 5.9307295e-05]\n",
            "Thresholds: [0.5437465  0.354477   0.31175053 0.6573511  0.45488623]\n",
            "F1-macro: 0.6740875614589578\n",
            "Precision-macro: 0.6819480008262498\n",
            "Recall-macro: 0.7032360471070149\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.67      0.50      0.57        16\n",
            "        fear       0.60      0.97      0.74        63\n",
            "         joy       0.56      0.77      0.65        31\n",
            "     sadness       0.85      0.63      0.72        35\n",
            "    surprise       0.74      0.65      0.69        31\n",
            "\n",
            "   micro avg       0.64      0.77      0.70       176\n",
            "   macro avg       0.68      0.70      0.67       176\n",
            "weighted avg       0.67      0.77      0.70       176\n",
            " samples avg       0.64      0.71      0.64       176\n",
            "\n",
            "Epoch 6/10, Loss: 0.0000, Per-class Loss: [1.2922884e-05 5.3643755e-05 3.7031590e-05 5.6571891e-05 3.7831138e-05]\n",
            "Thresholds: [0.46070293 0.3362851  0.4489241  0.5519298  0.5785042 ]\n",
            "F1-macro: 0.6815080491581955\n",
            "Precision-macro: 0.6950988582567531\n",
            "Recall-macro: 0.6928725038402457\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.67      0.50      0.57        16\n",
            "        fear       0.63      0.95      0.76        63\n",
            "         joy       0.61      0.71      0.66        31\n",
            "     sadness       0.85      0.66      0.74        35\n",
            "    surprise       0.71      0.65      0.68        31\n",
            "\n",
            "   micro avg       0.67      0.76      0.71       176\n",
            "   macro avg       0.70      0.69      0.68       176\n",
            "weighted avg       0.69      0.76      0.71       176\n",
            " samples avg       0.68      0.70      0.66       176\n",
            "\n",
            "Epoch 7/10, Loss: 0.0000, Per-class Loss: [8.3904142e-06 5.2216463e-05 3.0786246e-05 4.5083962e-05 3.3141398e-05]\n",
            "Thresholds: [0.55470866 0.45679006 0.34376815 0.48963562 0.561777  ]\n",
            "F1-macro: 0.6644152079343162\n",
            "Precision-macro: 0.6731509625126646\n",
            "Recall-macro: 0.6829121863799283\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.70      0.44      0.54        16\n",
            "        fear       0.63      0.94      0.75        63\n",
            "         joy       0.57      0.65      0.61        31\n",
            "     sadness       0.80      0.69      0.74        35\n",
            "    surprise       0.67      0.71      0.69        31\n",
            "\n",
            "   micro avg       0.65      0.75      0.70       176\n",
            "   macro avg       0.67      0.68      0.66       176\n",
            "weighted avg       0.67      0.75      0.69       176\n",
            " samples avg       0.65      0.68      0.63       176\n",
            "\n",
            "Epoch 8/10, Loss: 0.0000, Per-class Loss: [8.5336960e-06 4.5745634e-05 2.6723468e-05 3.8976374e-05 2.4802786e-05]\n",
            "Thresholds: [0.14866811 0.45295984 0.5762583  0.45984548 0.3607728 ]\n",
            "F1-macro: 0.6682782605363251\n",
            "Precision-macro: 0.6589568402611881\n",
            "Recall-macro: 0.7075089605734768\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.48      0.62      0.54        16\n",
            "        fear       0.64      0.94      0.76        63\n",
            "         joy       0.76      0.52      0.62        31\n",
            "     sadness       0.80      0.69      0.74        35\n",
            "    surprise       0.62      0.77      0.69        31\n",
            "\n",
            "   micro avg       0.66      0.76      0.70       176\n",
            "   macro avg       0.66      0.71      0.67       176\n",
            "weighted avg       0.67      0.76      0.70       176\n",
            " samples avg       0.65      0.68      0.63       176\n",
            "\n",
            "Epoch 9/10, Loss: 0.0000, Per-class Loss: [6.9508474e-06 4.5910023e-05 1.9905734e-05 2.9943627e-05 2.3749564e-05]\n",
            "Thresholds: [0.58640736 0.48373526 0.2184335  0.42308146 0.40151805]\n",
            "F1-macro: 0.6702239180986386\n",
            "Precision-macro: 0.6293546763314205\n",
            "Recall-macro: 0.7438440860215053\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.64      0.44      0.52        16\n",
            "        fear       0.66      0.90      0.77        63\n",
            "         joy       0.50      0.81      0.62        31\n",
            "     sadness       0.69      0.83      0.75        35\n",
            "    surprise       0.66      0.74      0.70        31\n",
            "\n",
            "   micro avg       0.63      0.80      0.70       176\n",
            "   macro avg       0.63      0.74      0.67       176\n",
            "weighted avg       0.64      0.80      0.70       176\n",
            " samples avg       0.64      0.72      0.65       176\n",
            "\n",
            "Early stopping triggered.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For hindi and english on multi-lingual"
      ],
      "metadata": {
        "id": "QxKOsC_tTZ-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# XLM-R BASED EMOTION CLASSIFIER\n",
        "# ==========================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, precision_recall_curve\n",
        "\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 2e-5\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "EARLY_STOPPING_PATIENCE = 3\n",
        "\n",
        "# Define emotions\n",
        "emotions_eng = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n",
        "emotions_hin = ['anger', 'fear', 'joy', 'sadness', 'surprise', 'disgust']  # Hindi includes 'disgust'\n",
        "\n",
        "# Whether or not to include disgust\n",
        "INCLUDE_DISGUST = True\n",
        "emotions = emotions_eng + ['disgust'] if INCLUDE_DISGUST else emotions_eng\n",
        "\n",
        "# Dataset class\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.FloatTensor(label)\n",
        "        }\n",
        "\n",
        "# XLM-R model class\n",
        "class EmotionClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(EmotionClassifier, self).__init__()\n",
        "        self.xlm_roberta = XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.out = nn.Linear(self.xlm_roberta.config.hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.xlm_roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        output = self.drop(pooled_output)\n",
        "        return self.out(output)\n",
        "\n",
        "# Focal Loss\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = self.bce(inputs, targets)\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        focal_loss = self.alpha * ((1 - pt) ** self.gamma) * BCE_loss\n",
        "        return focal_loss.mean(dim=0), focal_loss.mean()\n",
        "\n",
        "# Training epoch\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
        "    model = model.train()\n",
        "    total_loss = 0\n",
        "    per_class_loss = torch.zeros(len(emotions)).to(device)\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss_per_class, loss = loss_fn(outputs, labels)\n",
        "        per_class_loss += loss_per_class.detach()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader), (per_class_loss / len(data_loader)).cpu().numpy()\n",
        "\n",
        "# Evaluation\n",
        "def eval_model(model, data_loader, device):\n",
        "    model = model.eval()\n",
        "    predictions = []\n",
        "    real_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            real_labels.extend(labels.cpu().numpy())\n",
        "    return np.array(predictions), np.array(real_labels)\n",
        "\n",
        "# Getting weighted sampler\n",
        "def get_weighted_sampler(labels):\n",
        "    label_counts = np.sum(labels, axis=0)\n",
        "    class_weights = 1. / (label_counts + 1e-6)\n",
        "    sample_weights = []\n",
        "    for label in labels:\n",
        "        weight = np.sum(class_weights[label == 1])\n",
        "        sample_weights.append(weight)\n",
        "    return WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
        "\n",
        "# Get optimal thresholds\n",
        "def get_optimal_thresholds(preds, targets):\n",
        "    thresholds = []\n",
        "    for i in range(preds.shape[1]):\n",
        "        precision, recall, thresh = precision_recall_curve(targets[:, i], preds[:, i])\n",
        "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "        best_thresh = thresh[np.argmax(f1)] if len(thresh) > 0 else 0.5\n",
        "        thresholds.append(best_thresh)\n",
        "    return np.array(thresholds)\n",
        "\n",
        "def load_data(file_path, emotions, language):\n",
        "    df = pd.read_csv(file_path)\n",
        "    texts = df['text'].tolist()\n",
        "    # Check if 'disgust' is in the dataframe and add if missing\n",
        "    if 'disgust' not in df.columns:\n",
        "        df['disgust'] = 0\n",
        "    labels = df[emotions].values\n",
        "    return texts, labels\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
        "\n",
        "    # Define emotions list for both languages\n",
        "    emotions_eng = ['anger', 'fear', 'joy', 'sadness', 'surprise', 'disgust']  # Make sure \"disgust\" is here for consistency\n",
        "    emotions_hin = ['anger', 'fear', 'joy', 'sadness', 'surprise', 'disgust']\n",
        "\n",
        "    # Load both English and Hindi data\n",
        "    train_texts_eng, train_labels_eng = load_data('train_eng.csv', emotions_eng, 'eng')\n",
        "    train_texts_hin, train_labels_hin = load_data('train_hin.csv', emotions_hin, 'hin')\n",
        "    val_texts_eng, val_labels_eng = load_data('dev_eng.csv', emotions_eng, 'eng')\n",
        "    val_texts_hin, val_labels_hin = load_data('dev_hin.csv', emotions_hin, 'hin')\n",
        "\n",
        "    # Ensure both train and validation labels have the same number of emotion columns\n",
        "    # Add zeros for the 'disgust' column in the English data if it's missing\n",
        "    if train_labels_eng.shape[1] < len(emotions_eng):\n",
        "        train_labels_eng = np.hstack([train_labels_eng, np.zeros((train_labels_eng.shape[0], 1))])\n",
        "    if val_labels_eng.shape[1] < len(emotions_eng):\n",
        "        val_labels_eng = np.hstack([val_labels_eng, np.zeros((val_labels_eng.shape[0], 1))])\n",
        "\n",
        "    # Concatenate English and Hindi data\n",
        "    train_texts = train_texts_eng + train_texts_hin\n",
        "    train_labels = np.concatenate([train_labels_eng, train_labels_hin], axis=0)\n",
        "    val_texts = val_texts_eng + val_texts_hin\n",
        "    val_labels = np.concatenate([val_labels_eng, val_labels_hin], axis=0)\n",
        "\n",
        "    train_dataset = EmotionDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
        "    val_dataset = EmotionDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
        "\n",
        "    sampler = get_weighted_sampler(train_labels)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    model = EmotionClassifier(n_classes=len(emotions_eng)).to(DEVICE)\n",
        "    loss_fn = FocalLoss(gamma=2)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=1, verbose=True)\n",
        "\n",
        "    best_macro_f1 = 0\n",
        "    patience = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_loss, class_losses = train_epoch(model, train_loader, loss_fn, optimizer, DEVICE)\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {train_loss:.4f}, Per-class Loss: {class_losses}\")\n",
        "\n",
        "        preds, targets = eval_model(model, val_loader, DEVICE)\n",
        "        thresholds = get_optimal_thresholds(preds, targets)\n",
        "        bin_preds = (preds >= thresholds).astype(int)\n",
        "\n",
        "        macro_f1 = f1_score(targets, bin_preds, average='macro')\n",
        "        scheduler.step(macro_f1)\n",
        "\n",
        "        print(f\"F1-macro: {macro_f1}\")\n",
        "        print(\"Precision-macro:\", precision_score(targets, bin_preds, average='macro'))\n",
        "        print(\"Recall-macro:\", recall_score(targets, bin_preds, average='macro'))\n",
        "        print(\"Detailed Classification Report:\")\n",
        "        print(classification_report(targets, bin_preds, target_names=emotions_eng, zero_division=0))\n",
        "\n",
        "        if macro_f1 > best_macro_f1:\n",
        "            best_macro_f1 = macro_f1\n",
        "            patience = 0\n",
        "            torch.save(model.state_dict(), \"best_emotion_model.pt\")\n",
        "        else:\n",
        "            patience += 1\n",
        "            if patience >= EARLY_STOPPING_PATIENCE:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8126437a0c214cc4b5f6d5a84fee147d",
            "80e10ea8f49d43549e70867441d32cca",
            "f3219a34a6c74779be01e3e05a57888e",
            "960302d6b9a245309ccd9111b1f78d3e",
            "5fe02dbda66947ae8395aa855b4584e5",
            "b5f2ad4a3b0c4c8b97760e7f4915f574",
            "27911889a9344f6289474d2e55ea93f5",
            "3d7d4c4e85ab4d63a260ec957ac06364",
            "0066083e86dc496fbd3170a9968f8774",
            "2d7c8b8ef5af43d18ea98ffe0d434347",
            "ec5190b1f40d46439ad415037774bd32"
          ]
        },
        "id": "44mveuKQ93pa",
        "outputId": "6afa4b35-b822-4354-aec2-0fe675004c68"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8126437a0c214cc4b5f6d5a84fee147d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.1177, Per-class Loss: [0.14062075 0.11637207 0.11197977 0.1459718  0.11996376 0.07106446]\n",
            "F1-macro: 0.6947841833785517\n",
            "Precision-macro: 0.6711391197726488\n",
            "Recall-macro: 0.7278013653013654\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.69      0.62      0.66        32\n",
            "        fear       0.72      0.82      0.76        77\n",
            "         joy       0.49      0.64      0.56        42\n",
            "     sadness       0.60      0.73      0.66        52\n",
            "    surprise       0.64      0.75      0.69        40\n",
            "     disgust       0.89      0.80      0.84        10\n",
            "\n",
            "   micro avg       0.64      0.74      0.68       253\n",
            "   macro avg       0.67      0.73      0.69       253\n",
            "weighted avg       0.65      0.74      0.69       253\n",
            " samples avg       0.56      0.60      0.56       253\n",
            "\n",
            "Epoch 2/10, Loss: 0.0709, Per-class Loss: [0.08657888 0.08257655 0.05786232 0.10094671 0.07465521 0.02289892]\n",
            "F1-macro: 0.7055262353100412\n",
            "Precision-macro: 0.7025968228132715\n",
            "Recall-macro: 0.7202256077256077\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.87      0.62      0.73        32\n",
            "        fear       0.72      0.84      0.78        77\n",
            "         joy       0.62      0.57      0.59        42\n",
            "     sadness       0.64      0.73      0.68        52\n",
            "    surprise       0.70      0.75      0.72        40\n",
            "     disgust       0.67      0.80      0.73        10\n",
            "\n",
            "   micro avg       0.70      0.73      0.71       253\n",
            "   macro avg       0.70      0.72      0.71       253\n",
            "weighted avg       0.70      0.73      0.71       253\n",
            " samples avg       0.59      0.60      0.58       253\n",
            "\n",
            "Epoch 3/10, Loss: 0.0525, Per-class Loss: [0.0621866  0.06046878 0.04492232 0.07757197 0.05176691 0.0178673 ]\n",
            "F1-macro: 0.7362053829504903\n",
            "Precision-macro: 0.6907923203667884\n",
            "Recall-macro: 0.7904647435897436\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.71      0.78      0.75        32\n",
            "        fear       0.70      0.86      0.77        77\n",
            "         joy       0.64      0.64      0.64        42\n",
            "     sadness       0.59      0.71      0.64        52\n",
            "    surprise       0.68      0.85      0.76        40\n",
            "     disgust       0.82      0.90      0.86        10\n",
            "\n",
            "   micro avg       0.67      0.78      0.72       253\n",
            "   macro avg       0.69      0.79      0.74       253\n",
            "weighted avg       0.67      0.78      0.72       253\n",
            " samples avg       0.60      0.63      0.60       253\n",
            "\n",
            "Epoch 4/10, Loss: 0.0396, Per-class Loss: [0.04239241 0.04890273 0.03683611 0.06159793 0.03827466 0.00933361]\n",
            "F1-macro: 0.7120335622954483\n",
            "Precision-macro: 0.6606904017167362\n",
            "Recall-macro: 0.7863487207237206\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.66      0.84      0.74        32\n",
            "        fear       0.71      0.87      0.78        77\n",
            "         joy       0.45      0.76      0.57        42\n",
            "     sadness       0.57      0.69      0.63        52\n",
            "    surprise       0.68      0.75      0.71        40\n",
            "     disgust       0.89      0.80      0.84        10\n",
            "\n",
            "   micro avg       0.62      0.79      0.70       253\n",
            "   macro avg       0.66      0.79      0.71       253\n",
            "weighted avg       0.64      0.79      0.70       253\n",
            " samples avg       0.61      0.65      0.61       253\n",
            "\n",
            "Epoch 5/10, Loss: 0.0318, Per-class Loss: [0.02909315 0.0440909  0.03081699 0.04741183 0.03227402 0.00727237]\n",
            "F1-macro: 0.731756255327307\n",
            "Precision-macro: 0.7033320015571537\n",
            "Recall-macro: 0.7774055805305805\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.76      0.78      0.77        32\n",
            "        fear       0.65      0.97      0.78        77\n",
            "         joy       0.68      0.55      0.61        42\n",
            "     sadness       0.67      0.71      0.69        52\n",
            "    surprise       0.67      0.85      0.75        40\n",
            "     disgust       0.80      0.80      0.80        10\n",
            "\n",
            "   micro avg       0.68      0.80      0.73       253\n",
            "   macro avg       0.70      0.78      0.73       253\n",
            "weighted avg       0.68      0.80      0.73       253\n",
            " samples avg       0.62      0.65      0.62       253\n",
            "\n",
            "Epoch 6/10, Loss: 0.0204, Per-class Loss: [0.01800484 0.02996505 0.01901085 0.03002272 0.02142955 0.00394191]\n",
            "F1-macro: 0.7405084213014496\n",
            "Precision-macro: 0.7215041028066237\n",
            "Recall-macro: 0.7791212259962261\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.69      0.84      0.76        32\n",
            "        fear       0.69      0.91      0.78        77\n",
            "         joy       0.79      0.52      0.63        42\n",
            "     sadness       0.67      0.67      0.67        52\n",
            "    surprise       0.67      0.82      0.74        40\n",
            "     disgust       0.82      0.90      0.86        10\n",
            "\n",
            "   micro avg       0.70      0.77      0.73       253\n",
            "   macro avg       0.72      0.78      0.74       253\n",
            "weighted avg       0.70      0.77      0.73       253\n",
            " samples avg       0.62      0.64      0.61       253\n",
            "\n",
            "Epoch 7/10, Loss: 0.0160, Per-class Loss: [0.0121924  0.02557241 0.0155031  0.02517123 0.01441042 0.00322184]\n",
            "F1-macro: 0.7364566390400472\n",
            "Precision-macro: 0.6833849043493366\n",
            "Recall-macro: 0.8067730880230881\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.76      0.81      0.79        32\n",
            "        fear       0.70      0.91      0.79        77\n",
            "         joy       0.60      0.62      0.61        42\n",
            "     sadness       0.65      0.75      0.70        52\n",
            "    surprise       0.71      0.75      0.73        40\n",
            "     disgust       0.67      1.00      0.80        10\n",
            "\n",
            "   micro avg       0.68      0.79      0.73       253\n",
            "   macro avg       0.68      0.81      0.74       253\n",
            "weighted avg       0.68      0.79      0.73       253\n",
            " samples avg       0.63      0.66      0.63       253\n",
            "\n",
            "Epoch 8/10, Loss: 0.0133, Per-class Loss: [0.00879153 0.02143125 0.01192615 0.02243242 0.01227644 0.00280823]\n",
            "F1-macro: 0.731999745170053\n",
            "Precision-macro: 0.708459949090167\n",
            "Recall-macro: 0.7768960206460207\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.79      0.81      0.80        32\n",
            "        fear       0.65      0.95      0.77        77\n",
            "         joy       0.47      0.71      0.57        42\n",
            "     sadness       0.70      0.71      0.70        52\n",
            "    surprise       0.78      0.78      0.78        40\n",
            "     disgust       0.88      0.70      0.78        10\n",
            "\n",
            "   micro avg       0.66      0.81      0.72       253\n",
            "   macro avg       0.71      0.78      0.73       253\n",
            "weighted avg       0.67      0.81      0.73       253\n",
            " samples avg       0.64      0.67      0.63       253\n",
            "\n",
            "Epoch 9/10, Loss: 0.0095, Per-class Loss: [0.00647778 0.01600036 0.00779364 0.01480115 0.00963377 0.00235196]\n",
            "F1-macro: 0.7327457124793372\n",
            "Precision-macro: 0.7083521781005656\n",
            "Recall-macro: 0.7633411727161726\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.79      0.72      0.75        32\n",
            "        fear       0.72      0.83      0.77        77\n",
            "         joy       0.55      0.67      0.60        42\n",
            "     sadness       0.65      0.79      0.71        52\n",
            "    surprise       0.74      0.78      0.76        40\n",
            "     disgust       0.80      0.80      0.80        10\n",
            "\n",
            "   micro avg       0.69      0.77      0.73       253\n",
            "   macro avg       0.71      0.76      0.73       253\n",
            "weighted avg       0.69      0.77      0.73       253\n",
            " samples avg       0.65      0.64      0.63       253\n",
            "\n",
            "Early stopping triggered.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For only english advanced model"
      ],
      "metadata": {
        "id": "Reu5FCnmThyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# XLM-R BASED EMOTION CLASSIFIER\n",
        "# ==========================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, precision_recall_curve\n",
        "\n",
        "# Set random seeds\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "set_seed()\n",
        "\n",
        "# Constants\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 2e-5\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "INCLUDE_DISGUST = False\n",
        "EARLY_STOPPING_PATIENCE = 3\n",
        "GRAD_CLIP = 1.0\n",
        "MODEL_NAME = 'xlm-roberta-base'\n",
        "\n",
        "emotions = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n",
        "if INCLUDE_DISGUST:\n",
        "    emotions.append('disgust')\n",
        "\n",
        "# Dataset\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'labels': torch.FloatTensor(label)\n",
        "        }\n",
        "\n",
        "# Model\n",
        "class EmotionClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(EmotionClassifier, self).__init__()\n",
        "        self.transformer = XLMRobertaModel.from_pretrained(MODEL_NAME)\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.out = nn.Linear(self.transformer.config.hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token output\n",
        "        return self.out(self.drop(cls_output))\n",
        "\n",
        "# Focal Loss\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = torch.tensor(alpha).float() if alpha is not None else None\n",
        "        self.gamma = gamma\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = self.bce(inputs, targets)\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        focal_loss = ((1 - pt) ** self.gamma) * BCE_loss\n",
        "        if self.alpha is not None:\n",
        "            focal_loss = self.alpha.to(inputs.device) * focal_loss\n",
        "        return focal_loss.mean(dim=0), focal_loss.mean()\n",
        "\n",
        "# Train function\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    per_class_loss = torch.zeros(len(emotions)).to(device)\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss_per_class, loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        per_class_loss += loss_per_class.detach()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader), (per_class_loss / len(data_loader)).cpu().numpy()\n",
        "\n",
        "# Eval function\n",
        "def eval_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    real_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            real_labels.extend(labels.cpu().numpy())\n",
        "    return np.array(predictions), np.array(real_labels)\n",
        "\n",
        "# Sampler\n",
        "def get_weighted_sampler(labels):\n",
        "    label_counts = np.sum(labels, axis=0)\n",
        "    class_weights = 1. / (label_counts + 1e-6)\n",
        "    sample_weights = np.dot(labels, class_weights)\n",
        "    return WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True), class_weights\n",
        "\n",
        "# Threshold optimizer\n",
        "def get_optimal_thresholds(preds, targets):\n",
        "    thresholds = []\n",
        "    for i in range(preds.shape[1]):\n",
        "        precision, recall, thresh = precision_recall_curve(targets[:, i], preds[:, i])\n",
        "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "        best_thresh = thresh[np.argmax(f1)] if len(thresh) > 0 else 0.5\n",
        "        thresholds.append(best_thresh)\n",
        "    return np.array(thresholds)\n",
        "\n",
        "# Data loader\n",
        "def load_data(file_path, emotions):\n",
        "    df = pd.read_csv(file_path)\n",
        "    texts = df['text'].tolist()\n",
        "    labels = df[emotions].values\n",
        "    return texts, labels\n",
        "\n",
        "# Main loop\n",
        "def main():\n",
        "    tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_NAME)\n",
        "    train_texts, train_labels = load_data('train_eng.csv', emotions)\n",
        "    val_texts, val_labels = load_data('dev_eng.csv', emotions)\n",
        "\n",
        "    sampler, alpha = get_weighted_sampler(train_labels)\n",
        "    train_dataset = EmotionDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
        "    val_dataset = EmotionDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    model = EmotionClassifier(n_classes=len(emotions)).to(DEVICE)\n",
        "    loss_fn = FocalLoss(alpha=alpha, gamma=2)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "    best_macro_f1 = 0\n",
        "    patience = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_loss, class_losses = train_epoch(model, train_loader, loss_fn, optimizer, DEVICE)\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {train_loss:.4f}, Per-class Loss: {class_losses}\")\n",
        "\n",
        "        preds, targets = eval_model(model, val_loader, DEVICE)\n",
        "        thresholds = get_optimal_thresholds(preds, targets)\n",
        "        print(\"Thresholds:\", thresholds)\n",
        "        bin_preds = (preds >= thresholds).astype(int)\n",
        "\n",
        "        macro_f1 = f1_score(targets, bin_preds, average='macro')\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"F1-macro: {macro_f1}\")\n",
        "        print(\"Precision-macro:\", precision_score(targets, bin_preds, average='macro'))\n",
        "        print(\"Recall-macro:\", recall_score(targets, bin_preds, average='macro'))\n",
        "        print(\"Classification Report:\")\n",
        "        print(classification_report(targets, bin_preds, target_names=emotions, zero_division=0))\n",
        "\n",
        "        if macro_f1 > best_macro_f1:\n",
        "            best_macro_f1 = macro_f1\n",
        "            patience = 0\n",
        "            torch.save(model.state_dict(), \"best_emotion_model.pt\")\n",
        "        else:\n",
        "            patience += 1\n",
        "            if patience >= EARLY_STOPPING_PATIENCE:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZwEfyqrCwh7",
        "outputId": "49e09585-ed20-4952-e911-f05e6a04912d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.0003, Per-class Loss: [0.00046677 0.00010267 0.0002369  0.00022577 0.00022014]\n",
            "Thresholds: [0.52154803 0.48535854 0.3900765  0.5373386  0.48674816]\n",
            "F1-macro: 0.633762202248146\n",
            "Precision-macro: 0.6099200318934384\n",
            "Recall-macro: 0.7332821300563237\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.89      0.50      0.64        16\n",
            "        fear       0.59      0.97      0.73        63\n",
            "         joy       0.62      0.65      0.63        31\n",
            "     sadness       0.54      0.71      0.62        35\n",
            "    surprise       0.40      0.84      0.54        31\n",
            "\n",
            "   micro avg       0.55      0.80      0.65       176\n",
            "   macro avg       0.61      0.73      0.63       176\n",
            "weighted avg       0.58      0.80      0.65       176\n",
            " samples avg       0.56      0.72      0.60       176\n",
            "\n",
            "Epoch 2/10, Loss: 0.0002, Per-class Loss: [2.8881832e-04 8.9925001e-05 1.7144826e-04 1.7157492e-04 1.7703226e-04]\n",
            "Thresholds: [0.41819826 0.47349513 0.39110902 0.47692826 0.6108879 ]\n",
            "F1-macro: 0.7094548052043976\n",
            "Precision-macro: 0.6254965986394558\n",
            "Recall-macro: 0.828367895545315\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.52      0.81      0.63        16\n",
            "        fear       0.61      0.95      0.75        63\n",
            "         joy       0.63      0.71      0.67        31\n",
            "     sadness       0.64      0.83      0.72        35\n",
            "    surprise       0.72      0.84      0.78        31\n",
            "\n",
            "   micro avg       0.63      0.85      0.72       176\n",
            "   macro avg       0.63      0.83      0.71       176\n",
            "weighted avg       0.63      0.85      0.72       176\n",
            " samples avg       0.64      0.76      0.67       176\n",
            "\n",
            "Epoch 3/10, Loss: 0.0001, Per-class Loss: [1.9531720e-04 8.3216772e-05 1.3190320e-04 1.5551920e-04 1.4993853e-04]\n",
            "Thresholds: [0.40071523 0.5084099  0.52638245 0.5498959  0.5459514 ]\n",
            "F1-macro: 0.7016399056797861\n",
            "Precision-macro: 0.64995115995116\n",
            "Recall-macro: 0.7719495647721454\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.55      0.69      0.61        16\n",
            "        fear       0.64      0.92      0.76        63\n",
            "         joy       0.71      0.65      0.68        31\n",
            "     sadness       0.70      0.80      0.75        35\n",
            "    surprise       0.64      0.81      0.71        31\n",
            "\n",
            "   micro avg       0.65      0.81      0.72       176\n",
            "   macro avg       0.65      0.77      0.70       176\n",
            "weighted avg       0.66      0.81      0.72       176\n",
            " samples avg       0.66      0.73      0.67       176\n",
            "\n",
            "Epoch 4/10, Loss: 0.0001, Per-class Loss: [1.17776406e-04 7.48874663e-05 1.09506749e-04 1.31144217e-04\n",
            " 1.24334940e-04]\n",
            "Thresholds: [0.22076951 0.5359449  0.5264356  0.64763105 0.58181906]\n",
            "F1-macro: 0.6975317531305902\n",
            "Precision-macro: 0.6822237149823357\n",
            "Recall-macro: 0.7591551459293394\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.44      0.75      0.56        16\n",
            "        fear       0.66      0.90      0.76        63\n",
            "         joy       0.90      0.61      0.73        31\n",
            "     sadness       0.79      0.66      0.72        35\n",
            "    surprise       0.61      0.87      0.72        31\n",
            "\n",
            "   micro avg       0.66      0.78      0.72       176\n",
            "   macro avg       0.68      0.76      0.70       176\n",
            "weighted avg       0.70      0.78      0.72       176\n",
            " samples avg       0.65      0.69      0.64       176\n",
            "\n",
            "Epoch 5/10, Loss: 0.0001, Per-class Loss: [6.41689767e-05 6.76350974e-05 7.44218705e-05 1.20900535e-04\n",
            " 1.00385812e-04]\n",
            "Thresholds: [0.29381397 0.48461118 0.46552852 0.6718797  0.5450892 ]\n",
            "F1-macro: 0.705502387444669\n",
            "Precision-macro: 0.6638058098939464\n",
            "Recall-macro: 0.7760931899641577\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.48      0.75      0.59        16\n",
            "        fear       0.65      0.97      0.78        63\n",
            "         joy       0.80      0.65      0.71        31\n",
            "     sadness       0.68      0.74      0.71        35\n",
            "    surprise       0.71      0.77      0.74        31\n",
            "\n",
            "   micro avg       0.66      0.81      0.73       176\n",
            "   macro avg       0.66      0.78      0.71       176\n",
            "weighted avg       0.68      0.81      0.73       176\n",
            " samples avg       0.67      0.73      0.68       176\n",
            "\n",
            "Early stopping triggered.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZjaS_mGbIeAi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}